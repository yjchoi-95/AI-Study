# AI-Study

인공지능 논문 스터디

#### 1. 스터디 안내  
- 일시: 매주 수요일 9:00 PM  
- 내용: 연구 주제 발표 및 논문 리뷰  
- 목적: 다양한 분야의 최신 트렌드 파악 및 AI 지식 저변 확대
</br></br>
#### 2. 스터디 멤버  
- 임진혁: Knowledge Distillation, Meta Learning, Few Shot Learning, Self-supervised Learning, Domain Generalization, Federate Learning  
- 최영제: Machine Learning, Reinforcement Learning, Auto Feature Engineering, Time Series Forecasting, Anomaly Detection
</br></br>
### Paper List  

Date | Paper | Topic | Presenter | Links | Needs futher modification
---- | ---- | ---- | ---- | ---- | ----
2020.04.16 | [CVPR 2019] SpotTune, Transfer learning through adaptive fine-tuning | Vision, Transfer Learning | 최영제 | [paper](https://arxiv.org/pdf/1811.08737.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Transfer%20Learning/%5BCVPR%202019%5D%20SpotTune%2C%20Transfer%20learning%20through%20adaptive%20fine-tuning.pdf)</br>[blog](https://yjchoi-95.gitbook.io/paper-review/spottune-transfer-learning-through-adaptive-fine-tuning-cvpr-2019)| X
2020.04.23 | [NIPS 2015] Distilling the Knowledge in a Neural Network | Knowledge Distillation | 임진혁 |[paper](https://arxiv.org/pdf/1503.02531.pdf) </br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Knowledge%20Distillation/%5BNIPS%202015%5D%20Distilling%20the%20Knowledge%20in%20a%20Neural%20Network_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  | O
2020.05.14 | [CVPR 2019] Class-Balanced Loss Based on Effective Number of Samples | Class Imbalance | 최영제 | [paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Class%20imbalance/%5BCVPR%202019%5D%20Class-Balanced%20Loss%20Based%20on%20Effective%20Number%20of%20Samples.pdf)</br>[blog](https://yjchoi-95.gitbook.io/paper-review/cvpr-2019-class-balanced-loss-based-on-effective-number-of-samples) | X
2020.05.20 | [KDD 2018] Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System | Recommeder System</br> Knowledge Distillation | 임진혁|[paper](https://arxiv.org/pdf/1809.07428.pdf) </br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Knowledge%20Distillation/%5BKDD%202018%5D%20Ranking%20Distillation%20Learning%20Compact%20Ranking%20Models%20With%20High%20Performance%20for%20Recommender%20System_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  | O 
2020.07.02 | [NIPS 2019] Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models | Time Series Forecasting | 최영제 | [paper](https://arxiv.org/pdf/1909.09020.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Time%20Series%20Forecasting/%5BNIPS%202019%5D%20Shape%20and%20Time%20Distortion%20Loss%20for%20Training%20Deep%20Time%20Series%20Forecasting%20Models.pdf) | X
2020.07.08 | [ICML 2020] Rethinking Data Augmentation: Self-Supervision and Self-Distillation | Augmentation</br> Self-Supervised Learning | 임진혁 | [paper](https://openreview.net/pdf?id=SkliR1SKDS) </br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/SSL/%5BICML%202020%5DRethinking%20Data%20Augmentation%20Self-Supervision%20and%20Self-Distillation_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  | O 
2020.07.16 | [ICLR 2020] Distance based learning from errors for confidence calibration | Model Calibration | 최영제 | [paper](https://arxiv.org/pdf/1912.01730.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Calibration-Distance%20based%20learning/%5BICLR%202020%5D%20Distance%20based%20learning%20from%20errors%20for%20confidence%20calibration.pdf)</br>[blog](https://yjchoi-95.gitbook.io/paper-review/iclr-2020-distance-based-learning-from-errors-for-confidence-calibration) | X
2020.07.23 | [NIPS 2019] Knowledge Extraction with No Observable Data | Knowledge Disillation | 임진혁 |[paper](https://papers.nips.cc/paper/2019/file/596f713f9a7376fe90a62abaaedecc2d-Paper.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Knowledge%20Distillation/%5BNIPS%202019%5D%20Knowledge%20Extraction%20with%20No%20Observable%20Data_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  | O  
2021.02.25	|[NIPS 2020] Self-supervised Auxiliary Learning with Meta-paths for Heterogeneous Graphs |	GNN</br> Self-Supervised Learning| 	임진혁|[paper](https://arxiv.org/pdf/2007.08294.pdf) </br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/SSL/%5BNIPS%202020%5D%20Self-supervised%20Auxiliary%20Learning%20with%20Meta-paths%20for%20Heterogeneous%20Graphs_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O
2021.02.25 | [ICML 2018] GAIN Missing Data Imputation using Generative Adversarial Nets | Data Imputation | 최영제 | [paper](https://arxiv.org/pdf/1806.02920.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Data%20imputation/%5BICML%202018%5D%20GAIN%20Missing%20Data%20Imputation%20using%20Generative%20Adversarial%20Nets.pdf)</br>[blog](https://yjchoi-95.gitbook.io/paper-review/icml-2018-gain-missing-data-imputation-using-generative-adversarial-nets) | X
2021.03.04 |	[AISTATS 2017] Communication-Efficient Learning of Deep Networks from Decentralized Data	| Federate Learning	| 임진혁|[paper](https://arxiv.org/pdf/1602.05629.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Federate%20Learning/%5BAISTATS%202017%5D%20Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O 
2021.03.11 | Reinforcement learning 01 - MDP, Q-learning | Reinforcement Learning | 최영제 | [review](https://github.com/yjchoi-95/AI-Study/blob/main/Reinforcement%20Learning/Reinforcement%20Learning%2001%20-%20MDP%2C%20Q-learning.pdf) | O
2021.03.18	| [NIPS 2019] FedMD: Heterogenous Federated Learning via Model Distillation	| Federate Learning, Knowledge Distillation|	임진혁	|[paper](https://arxiv.org/pdf/1910.03581.pdf)</br> [review](https://github.com/yjchoi-95/AI-Study/blob/main/Federate%20Learning/%5BNIPS%202019%5D%20FedMD%20Heterogenous%20Federated%20Learning%20via%20Model%20Distillation_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |	O
2021.03.18 | Reinforcement learning 02 - DQN, PER, Dueling DQN | Reinforcement Learning | 최영제 | [review](https://github.com/yjchoi-95/AI-Study/blob/main/Reinforcement%20Learning/Reinforcement%20Learning%2002%20-%20DQN%2C%20PER%2C%20Dueling%20DQN.pdf)</br>[blog](https://yjchoi-95.gitbook.io/paper-review/rl-background/arxiv-2013-playing-atari-with-deep-reinforcement-learning) | X
 2021.03.25	| [ICML 2017] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks	| Meta Learning</br> Few Shot| 	임진혁|[paper](http://proceedings.mlr.press/v70/finn17a/finn17a.pdf)</br> [review](https://github.com/yjchoi-95/AI-Study/blob/main/Meta%20Learning/%5BICML%202017%5D%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O
2021.03.25 | Reinforcement learning 03 - PG, Actor-critic, A2C | Reinforcement Learning | 최영제 | [review](https://github.com/yjchoi-95/AI-Study/blob/main/Reinforcement%20Learning/Reinforcement%20Learning%2003%20-%20PG%2C%20Actor-critic%2C%20A2C.pdf) | X
 2021.03.31	| [ICLR 20201] DOMAIN GENERALIZATION WITH MIXSTYLE | 	Domain Generalization	| 임진혁|[paper](https://arxiv.org/pdf/2104.02008.pdf)</br> [review](https://github.com/yjchoi-95/AI-Study/blob/main/Domain%20Generalization/%5BICLR%2020201%5D%20DOMAIN%20GENERALIZATION%20WITH%20MIXSTYLE_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O
2021.03.31 | Reinforcement learning 04 - A3C, SIL | Reinforcement Learning | 최영제 | [review](https://github.com/yjchoi-95/AI-Study/blob/main/Reinforcement%20Learning/Reinforcement%20Learning%2004%20-%20A3C%2C%20SIL.pdf) | X
 2021.04.07	| [arXiv 2021] Meta Pseudo Labels 	|Semi-Supervised Learning	|임진혁|[paper](https://arxiv.org/pdf/2003.10580.pdf) </br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/SSL/%5BarXiv%202021%5D%20Meta%20Pseudo%20Labels_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O
2021.04.07 | [ICLR 2017] Neural architecture search with reinforcement learning | AutoML | 최영제 | [paper](https://arxiv.org/pdf/1611.01578.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Auto%20ML/%5BICLR%202017%5D%20Neural%20architecture%20search%20with%20reinforcement%20learning.pdf) | X
 2021.05.12	|[CVPR 2020] Attentive Weights Generation for Few Shot Learning via Information Maximization	|Few Shot</br> Attention	|임진혁|[paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Attentive_Weights_Generation_for_Few_Shot_Learning_via_Information_Maximization_CVPR_2020_paper.pdf) </br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Meta%20Learning/%5BCVPR%202020%5D%20Attentive%20Weights%20Generation%20for%20Few%20Shot%20Learning%20via%20Information%20Maximization_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O
2021.05.12 | [ICML 2018] Efficient nerual architecture search via parameter sharing | AutoML | 최영제 | [paper](https://arxiv.org/pdf/1802.03268.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Auto%20ML/%5BICML%202018%5D%20Efficient%20nerual%20architecture%20search%20via%20parameter%20sharing.pdf) | X
 2021.05.25 |	[ICLR 2015] ADAM : A METHOD FOR STOCHASTIC OPTIMIZATION	|Optimization|	임진혁|[paper](https://arxiv.org/pdf/1412.6980.pdf) </br> [review](https://github.com/yjchoi-95/AI-Study/blob/main/Neural%20Network/%5BICLR%202015%5D%20ADAM%20%20A%20METHOD%20FOR%20STOCHASTIC%20OPTIMIZATION_%EC%9E%84%EC%A7%84%ED%98%81.pdf) 		| O 
2021.05.25 | [arXiv 2020] DIFER, Differentiable automated feature engineering | AutoFE | 최영제 | [paper](https://arxiv.org/pdf/2010.08784.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Auto%20FE/%5BarXiv%202020%5D%20DIFER%2C%20Differentiable%20automated%20feature%20engineering.pdf) | X
2021.06.09 | [KDD 2020] USAD, unsupervised anomaly detection on multivariate time series | Anomaly Detection | 최영제 | [paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403392)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Anomaly%20Detection/%5BKDD%202020%5D%20USAD%2C%20unsupervised%20anomaly%20detection%20on%20multivariate%20time%20series.pdf)</br>[blog](https://yjchoi-95.gitbook.io/paper-review/kdd-2020-usad-unsupervised-anomaly-detection-on-multivariate-time-series) | X
 2021.06.16	|[ICML 2019] Zero-Shot Knowledge Distillation in Deep Networks	| Knowledge Distillation</br> Few Shot|	임진혁|[paper](https://arxiv.org/pdf/1905.08114.pdf) </br> [review](https://github.com/yjchoi-95/AI-Study/blob/main/Knowledge%20Distillation/%5BICML%202019%5D%20Zero-Shot%20Knowledge%20Distillation%20in%20Deep%20Networks_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O
2021.06.23 |	[arXiv 2018] Federated Meta-Learning with Fast Convergence and Efficient Communication |	Federate Learning</br> Meta Learning |	임진혁	|[paper](https://arxiv.org/pdf/1802.07876.pdf) </br> [review](https://github.com/yjchoi-95/AI-Study/blob/main/Federate%20Learning/%5BarXiv%202018%5D%20Federated%20Meta-Learning%20with%20Fast%20Convergence%20and%20Efficient%20Communication_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |	O
2021.06.23 | [IEEE ICBD 2020] TadGAN, Time series anomaly detection using generative adversarial networks | Anomaly Detection | 최영제 | [paper](https://arxiv.org/pdf/2009.07769.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Anomaly%20Detection/%5BIEEE%20ICBD%202020%5D%20TadGAN%2C%20Time%20series%20anomaly%20detection%20using%20generative%20adversarial%20networks.pdf) | X
 2021.06.30	|[ICLR 2021] BOIL: TOWARDS REPRESENTATION CHANGE FOR FEW-SHOT LEARNING	|Meta Learning</br> Few Shot|	임진혁|[paper](https://arxiv.org/pdf/2008.08882.pdf) </br> [review](https://github.com/yjchoi-95/AI-Study/blob/main/Meta%20Learning/%5BICLR%202021%5D%20BOIL%20TOWARDS%20REPRESENTATION%20CHANGE%20FOR%20FEW-SHOT%20LEARNING_%EC%9E%84%EC%A7%84%ED%98%81.pdf)  |		O
2021.06.30 | [ICDM 2019] Neural feature search, A nueral architecture for automated feature enigneering | AutoFE | 최영제 | [paper](https://www.microsoft.com/en-us/research/uploads/prod/2019/09/NFS_ICDM_2019_camera_ready.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Auto%20FE/%5BICDM%202019%5D%20Neural%20feature%20search%2C%20A%20nueral%20architecture%20for%20automated%20feature%20enigneering.pdf) | X
2021.07.07 | [arXiv 2018] Exploration by Random Network Distillation | Reinforcement Learning | 최영제 | [paper](https://arxiv.org/pdf/1810.12894.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Reinforcement%20Learning/%5BarXiv%202018%5D%20Exploration%20by%20Random%20Network%20Distillation.pdf) | X
2021.07.16 | [NIPS 2019] Domain Generalization via Model-Agnostic Learning of Semantic Features | Domain Generalization</br>Meta Learning | 임진혁 | [paper](https://arxiv.org/pdf/1910.13580.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Domain%20Generalization/%5BNeurIPS%202019%5D%20Domain%20Generalization%20via%20Model-Agnostic%20Learning%20of%20Semantic%20Features_%EC%9E%84%EC%A7%84%ED%98%81.pdf) | X
2021.07.21 | [PAKDD 2020] Cross data Automatic Feature Engineering via Meta learning and Reinforcement Learning | AutoFE | 최영제 | [paper](https://www.springerprofessional.de/en/cross-data-automatic-feature-engineering-via-meta-learning-and-r/17965952)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Auto%20FE/%5BPAKDD%202020%5D%20Cross%20data%20Automatic%20Feature%20Engineering%20via%20Meta%20learning%20and%20Reinforcement%20Learning.pdf) | X
 | | | | |
2021.08.04 | [arXiv 2017] PathNet, Evolution Channels Gradient Descent in Super Neural Networks | Transfer Learning | 최영제 | [paper](https://arxiv.org/pdf/1701.08734.pdf)</br>[review](https://github.com/yjchoi-95/AI-Study/blob/main/Transfer%20Learning/%5BarXiv%202017%5D%20PathNet%2C%20Evolution%20Channels%20Gradient%20Descent%20in%20Super%20Neural%20Networks.pdf) | X
